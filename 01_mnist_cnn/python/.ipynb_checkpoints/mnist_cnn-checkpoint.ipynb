{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # conv1, 加padding扩充成32*32\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   # (n+2*p-k+1)/s = (28+2-3+1)/1=28\n",
    "        # conv2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # (n+2*p-k+1)/s = (14+2-3+1)/1=14\n",
    "    \n",
    "        # fc\n",
    "        self.fc1 = nn.Linear(7*7*32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv --> relu --> maxpool\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))   # 7*7*32\n",
    "        \n",
    "        # fc1 \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size torch.Size([60000, 28, 28])\n",
      "train label size torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "EPOCH = 10               # 训练epoch次数\n",
    "BATCH_SIZE = 64          # 批训练的数量\n",
    "LR = 0.001               # 学习率      \n",
    "\n",
    "train_data = datasets.MNIST(root='./', train=True,transform=transforms.ToTensor(), download=False)\n",
    "test_data = datasets.MNIST(root='./', train=False,transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255\n",
    "test_y = test_data.test_labels.numpy()[:2000]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"train size\", train_data.train_data.size())\n",
    "print(\"train label size\", train_data.train_labels.size())\n",
    "\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 loss:0.1057%\n",
      "epoch2 loss:0.0625%\n",
      "epoch3 loss:0.0912%\n",
      "epoch4 loss:0.0421%\n",
      "epoch5 loss:0.0209%\n",
      "epoch6 loss:0.0189%\n",
      "epoch7 loss:0.0081%\n",
      "epoch8 loss:0.0011%\n",
      "epoch9 loss:0.0098%\n",
      "epoch10 loss:0.0821%\n"
     ]
    }
   ],
   "source": [
    "# 使用dataloader进行分批\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# model\n",
    "model = LeNet5()\n",
    "# loss functino\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# device \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# train\n",
    "for epoch in range(EPOCH):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # gpu use\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        # loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # step 参数更新\n",
    "        optimizer.step()\n",
    "        \n",
    "    # 为什么用item，因为loss只有一个数值\n",
    "    print('epoch{} loss:{:.4f}%'.format(epoch+1, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 test correct:98.5000%\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'mnist_cnn.pt')\n",
    "model = torch.load('mnist_cnn.pt')\n",
    "\n",
    "# 测试\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # forward\n",
    "    out = model(images)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total = total + labels.size(0)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    \n",
    "# correct \n",
    "print('10000 test correct:{:.4f}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** total ******************************\n",
      "<bound method Module.load_state_dict of LeNet5(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")>\n",
      "****************************** conv1 ******************************\n",
      "torch.Size([16, 1, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0727, -0.6638, -0.4027],\n",
      "          [ 0.4068,  0.3675, -0.1370],\n",
      "          [ 0.2698,  0.3330,  0.1452]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5470,  0.2916,  0.5017],\n",
      "          [-0.3846, -0.1087, -0.1956],\n",
      "          [-0.1527, -0.3478, -0.3169]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1435,  0.3135,  0.2781],\n",
      "          [ 0.0490, -0.0038, -0.2238],\n",
      "          [ 0.2416,  0.1356, -0.1209]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1253,  0.2080, -0.4524],\n",
      "          [ 0.2282,  0.1175,  0.4227],\n",
      "          [ 0.2152,  0.0098,  0.1097]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3057,  0.5109,  0.4266],\n",
      "          [-0.0245, -0.0435,  0.4368],\n",
      "          [-0.6497, -0.6301, -0.3782]]],\n",
      "\n",
      "\n",
      "        [[[-0.0912,  0.4352,  0.3930],\n",
      "          [-0.6308, -0.3176,  0.3500],\n",
      "          [-0.4335, -0.7725, -0.7393]]],\n",
      "\n",
      "\n",
      "        [[[-0.4164,  0.3490,  0.2931],\n",
      "          [ 0.2118,  0.4676,  0.0854],\n",
      "          [-0.4588,  0.0611, -0.0308]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7338,  0.7829, -0.0170],\n",
      "          [ 0.2311, -0.1194, -0.5499],\n",
      "          [-0.0490, -0.5000, -0.6837]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1014,  0.2397,  0.0419],\n",
      "          [ 0.0818,  0.2562,  0.0161],\n",
      "          [ 0.1661,  0.3139,  0.2321]]],\n",
      "\n",
      "\n",
      "        [[[-0.4530, -0.6963, -0.1966],\n",
      "          [-0.5058, -0.0447, -0.2335],\n",
      "          [ 0.2290,  0.2665,  0.4952]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0639,  0.1248,  0.1362],\n",
      "          [ 0.3997,  0.1246, -0.0908],\n",
      "          [ 0.3943,  0.1138, -0.5753]]],\n",
      "\n",
      "\n",
      "        [[[-0.5404, -0.0640,  0.1658],\n",
      "          [-0.2866,  0.3313,  0.0133],\n",
      "          [ 0.4310,  0.3726, -0.1458]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3878,  0.2713, -0.0290],\n",
      "          [ 0.5060,  0.0959, -0.7200],\n",
      "          [ 0.4289,  0.0798,  0.0664]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1642, -0.1008, -0.8312],\n",
      "          [ 0.4024,  0.4895, -0.4123],\n",
      "          [ 0.3976,  0.4036, -0.1043]]],\n",
      "\n",
      "\n",
      "        [[[-0.4056, -0.3926, -0.3069],\n",
      "          [ 0.0455, -0.1559,  0.0290],\n",
      "          [ 0.5364,  0.3996,  0.2447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3245,  0.3010, -0.4186],\n",
      "          [-0.1465,  0.5026,  0.2034],\n",
      "          [-0.1003,  0.0363,  0.3725]]]], device='cuda:0', requires_grad=True)\n",
      "torch.Size([16])\n",
      "****************************** conv2 ******************************\n",
      "torch.Size([32, 16, 3, 3])\n",
      "torch.Size([32])\n",
      "****************************** fc1 ******************************\n",
      "torch.Size([128, 1568])\n",
      "torch.Size([128])\n",
      "****************************** fc2 ******************************\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('mnist_cnn.pt')\n",
    "print(30*\"*\"+\" total \" + 30*\"*\")\n",
    "print(model.load_state_dict)\n",
    "print(30*\"*\"+\" conv1 \" + 30*\"*\")\n",
    "print(model.conv1.weight.size())\n",
    "print(model.conv1.weight)\n",
    "print(model.conv1.bias.size())\n",
    "print(30*\"*\"+\" conv2 \" + 30*\"*\")\n",
    "print(model.conv2.weight.size())\n",
    "print(model.conv2.bias.size())\n",
    "print(30*\"*\"+\" fc1 \" + 30*\"*\")\n",
    "print(model.fc1.weight.size())\n",
    "print(model.fc1.bias.size())\n",
    "print(30*\"*\"+\" fc2 \" + 30*\"*\")\n",
    "print(model.fc2.weight.size())\n",
    "print(model.fc2.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Record_Tensor(tensor,name):\n",
    "\tprint (\"Recording tensor \"+name+\" ...\")\n",
    "\tf = open('./record/mnist_cnn/'+name+'.dat', 'w')\n",
    "\tarray=tensor.cpu().detach().numpy();\n",
    "\t#print (\"The range: [\"+str(np.min(array))+\":\"+str(np.max(array))+\"]\")\n",
    "\tprint(\"shape:{}\".format(np.shape(array)))\n",
    "\tif(np.size(np.shape(array))==1):\n",
    "\t\tRecord_Array1D(array,name,f)\n",
    "\telse:\n",
    "\t\tif(np.size(np.shape(array))==2):\n",
    "\t\t\tRecord_Array2D(array,name,f)\n",
    "\t\telse:\n",
    "\t\t\tif(np.size(np.shape(array))==3):\n",
    "\t\t\t\tRecord_Array3D(array,name,f)\n",
    "\t\t\telse:\n",
    "\t\t\t\tRecord_Array4D(array,name,f)\n",
    "\tf.close();\n",
    "\n",
    "def Record_Array1D(array,name,f):\n",
    "\tfor i in range(np.shape(array)[0]):\n",
    "\t\tf.write(str(array[i])+\"\\n\");\n",
    "\n",
    "def Record_Array2D(array,name,f):\n",
    "\tfor i in range(np.shape(array)[0]):\n",
    "\t\tfor j in range(np.shape(array)[1]):\n",
    "\t\t\tf.write(str(array[i][j])+\"\\n\");\n",
    "\n",
    "def Record_Array3D(array,name,f):\n",
    "\tfor i in range(np.shape(array)[0]):\n",
    "\t\tfor j in range(np.shape(array)[1]):\n",
    "\t\t\tfor k in range(np.shape(array)[2]):\n",
    "\t\t\t\tf.write(str(array[i][j][k])+\"\\n\");\n",
    "\n",
    "def Record_Array4D(array,name,f):\n",
    "\tfor i in range(np.shape(array)[0]):\n",
    "\t\tfor j in range(np.shape(array)[1]):\n",
    "\t\t\tfor k in range(np.shape(array)[2]):\n",
    "\t\t\t\tfor l in range(np.shape(array)[3]):\n",
    "\t\t\t\t\tf.write(str(array[i][j][k][l])+\"\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording tensor W_conv1 ...\n",
      "shape:(16, 1, 3, 3)\n",
      "Recording tensor b_conv1 ...\n",
      "shape:(16,)\n",
      "Recording tensor W_conv2 ...\n",
      "shape:(32, 16, 3, 3)\n",
      "Recording tensor b_conv2 ...\n",
      "shape:(32,)\n",
      "Recording tensor W_fc1 ...\n",
      "shape:(128, 1568)\n",
      "Recording tensor b_fc1 ...\n",
      "shape:(128,)\n",
      "Recording tensor W_fc2 ...\n",
      "shape:(10, 128)\n",
      "Recording tensor b_fc2 ...\n",
      "shape:(10,)\n"
     ]
    }
   ],
   "source": [
    "Record_Tensor(model.conv1.weight,\"W_conv1\")\n",
    "Record_Tensor(model.conv1.bias,\"b_conv1\")\n",
    "Record_Tensor(model.conv2.weight,\"W_conv2\")\n",
    "Record_Tensor(model.conv2.bias,\"b_conv2\")\n",
    "Record_Tensor(model.fc1.weight,\"W_fc1\")\n",
    "Record_Tensor(model.fc1.bias,\"b_fc1\")\n",
    "Record_Tensor(model.fc2.weight,\"W_fc2\")\n",
    "Record_Tensor(model.fc2.bias,\"b_fc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
